{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting resnet\n",
      "  Downloading resnet-0.1.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: keras>=2.0 in d:\\anaconda3\\lib\\site-packages (from resnet) (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\anaconda3\\lib\\site-packages (from keras>=2.0->resnet) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in d:\\anaconda3\\lib\\site-packages (from keras>=2.0->resnet) (1.19.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in d:\\anaconda3\\lib\\site-packages (from keras>=2.0->resnet) (1.0.8)\n",
      "Requirement already satisfied: h5py in d:\\anaconda3\\lib\\site-packages (from keras>=2.0->resnet) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in d:\\anaconda3\\lib\\site-packages (from keras>=2.0->resnet) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in d:\\anaconda3\\lib\\site-packages (from keras>=2.0->resnet) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in d:\\anaconda3\\lib\\site-packages (from keras>=2.0->resnet) (1.4.1)\n",
      "Building wheels for collected packages: resnet\n",
      "  Building wheel for resnet (setup.py): started\n",
      "  Building wheel for resnet (setup.py): finished with status 'done'\n",
      "  Created wheel for resnet: filename=resnet-0.1-py3-none-any.whl size=10047 sha256=9b01b0b8641d481f0f5125cba3603e395697d10aa3ededce6222d841d6a059d8\n",
      "  Stored in directory: c:\\users\\tut\\appdata\\local\\pip\\cache\\wheels\\38\\23\\58\\6e83e34ced7d4772b88e2e86305f94a1ca497bd87937f5e7ec\n",
      "Successfully built resnet\n",
      "Installing collected packages: resnet\n",
      "Successfully installed resnet-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image-classifiers\n",
      "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in d:\\anaconda3\\lib\\site-packages (from image-classifiers) (1.0.8)\n",
      "Requirement already satisfied: h5py in d:\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in d:\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (1.19.1)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\lib\\site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->image-classifiers) (1.15.0)\n",
      "Installing collected packages: image-classifiers\n",
      "Successfully installed image-classifiers-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install image-classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from resnet_builder import resnet # 這是從 resnet_builder.py 中直接 import 撰寫好的 resnet 函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "def resnet(input_shape, depth=29, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tensorflow.keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料集並作前處理\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   272         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 64)   1088        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 64)   0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   1040        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 64)   1088        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 64)   0           add[0][0]                        \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   1040        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 64)   1088        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   4160        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 128)  8320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 128)  0           conv2d_14[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 64)   8256        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 64)   36928       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 128)  8320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 128)  0           add_3[0][0]                      \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 64)   8256        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 64)   36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 128)  8320        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 128)    16512       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 128)    512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 128)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 128)    147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 256)    33024       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 256)    0           conv2d_24[0][0]                  \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 256)    1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 256)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 128)    32896       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 128)    512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 128)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 128)    147584      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 256)    33024       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 256)    0           add_6[0][0]                      \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 256)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 128)    32896       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 128)    512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 128)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 128)    147584      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 256)    33024       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 256)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 256)          0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           2570        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 849,002\n",
      "Trainable params: 843,786\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 建立 ResNet 模型\n",
    "model = resnet(input_shape=(32,32,3)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 47s 947us/sample - loss: 1.8300 - accuracy: 0.5143 - val_loss: 1.8840 - val_accuracy: 0.48486 - loss: 2.4841 - accuracy: 0. - ETA: 1:23 - loss: 2.465 - ETA: 1 - E\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 41s 816us/sample - loss: 1.3168 - accuracy: 0.6659 - val_loss: 1.9731 - val_accuracy: 0.4938 accura - ETA: 25s - loss: 1.3947 - accuracy:  - ETA: - ETA: 21 - ETA: 13s - los - ETA: 11s - loss: 1.3452 - accuracy: 0. - ETA: 10s - loss: 1.3447 - accuracy: 0.6 - ETA: 10s - loss: 1.3438 - ETA: 9s - loss: 1.3404 \n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 41s 817us/sample - loss: 1.1010 - accuracy: 0.7312 - val_loss: 1.7346 - val_accuracy: 0.5771\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 41s 817us/sample - loss: 0.9620 - accuracy: 0.7745 - val_loss: 1.3798 - val_accuracy: 0.6383loss: 0.9648 - accu - ETA: 23s - loss: 0.9663 -  - ETA: 13s - loss: 0.9642 - ac - ETA: 11s - loss: 0.9661 - accuracy: 0 - ETA:  - ETA: 8s - loss: 0.9648 - ac - E\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 41s 816us/sample - loss: 0.8712 - accuracy: 0.8062 - val_loss: 1.2846 - val_accuracy: 0.6710loss: 0.826 - ETA: 34s - loss: 0.8409 - accuracy: 0 - ETA: 3 - ETA: 30 - ETA: 27s - loss:\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 41s 816us/sample - loss: 0.8016 - accuracy: 0.8269 - val_loss: 1.0746 - val_accuracy: 0.7378oss: 0.7461 - ETA: 24s - loss: 0.7779 - accurac - ETA: 23s - loss: 0.7808 - acc - - ETA: 18s - loss: 0.785 - ETA: 4s - ETA: 2s - loss: 0.7998 - accuracy - ETA: 2s - loss: 0.8003 - accu\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 41s 815us/sample - loss: 0.7508 - accuracy: 0.8438 - val_loss: 1.2440 - val_accuracy: 0.699726s - loss: 0.7258 - accuracy: - ETA: 25s - loss: 0.7261 - a - ETA: 24s - loss: 0.7251 -  - ETA: 5s - l\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 41s 815us/sample - loss: 0.7050 - accuracy: 0.8612 - val_loss: 1.1030 - val_accuracy: 0.7356s - ETA: 28s - loss: 0.6649 - accurac - ETA: 27s - loss: 0.6661 - accuracy - ETA: 26s - loss: 0.6658 - ac - ETA: 25s - loss: 0.6687 - accur - ETA: 24s - los - ETA: 21s - loss: 0.6828 - ETA: 19s - loss: 0.6841 - ETA: 18s - loss: 0.6829 - accurac - ETA: 17s - loss: 0.6848 -  - ETA:  - ETA: 12s - loss: - ETA: 9s - loss: 0.6 - ETA - ETA: 6s - loss: - ETA: 3s - loss: 0.7014 -  - ETA: 2s - loss: 0.7020 - accuracy:  - ETA: 2s - loss: 0.7023  - ETA: \n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 41s 816us/sample - loss: 0.6704 - accuracy: 0.8735 - val_loss: 1.4045 - val_accuracy: 0.6712oss: 0.6376 - accuracy: - ETA: 33s - loss: 0.6360 - accuracy: -  - ETA: 24s - loss: 0 - ETA: 22s - loss: 0.6406 - accuracy: 0.8 - ETA: 21s - loss: 0.6404 - accuracy: 0.8 - ETA: 4s - loss: 0.6634 - accuracy:  - ETA: 4s - loss: 0.6642 - accura\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 41s 816us/sample - loss: 0.6382 - accuracy: 0.8851 - val_loss: 1.6891 - val_accuracy: 0.6523loss: - ETA: 33s - loss: 0.5863 - accuracy: 0. - ETA: 33s - loss: 0.5853  - ETA: 31s - loss: 0.5829 - accuracy - ETA: 30s - loss: 0.5810 - accuracy: - ETA: 30s - loss: 0.5810 - ac - ETA: 28s - loss: 0.5830 - accuracy: 0.9 - ETA: 28s - loss: 0.5831 - accuracy: - ETA: 28s - loss - E - ETA: 21s - loss: 0.6057 - accuracy:  - ETA: 20s - loss: 0.6066 - - ETA: 19s - loss: 0.6111 - accuracy: 0.894 - ETA: 19s - loss: 0.6111 - accurac - ETA: 18s -  - ETA: 15s - loss: 0. - ETA: 12s - - ETA: 9s - loss: 0.6247 - accurac - ETA: 9s - loss: 0.6260 - accu - ETA: 8s - loss: 0 - ETA: 7s - los - ETA: 4s - loss: 0.6327 - accuracy: 0. - E\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 41s 816us/sample - loss: 0.6095 - accuracy: 0.8954 - val_loss: 1.2593 - val_accuracy: 0.7325 ac - ETA: 33s - loss: 0.5819 -  - ETA: 32s - loss: 0.5715 - accuracy: 0.912 - ETA: 3 - ETA: 28s - loss: 0.5756 - accuracy: 0 - ETA: 28s - l - ETA:  - ETA: 21s - los - ETA: 19s -  - ETA: 16s - loss: 0.5924 - accuracy: 0.9 - ETA: 15s - loss: 0.5917 - accuracy: 0.901 - ETA: 15s - lo - ETA: 12s - loss: 0.5958 - accuracy: 0. - ETA: 12s - loss: 0.5962 - accuracy:   - ETA: 4s - loss: 0.6052  - ETA: 1s\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 41s 818us/sample - loss: 0.5939 - accuracy: 0.9018 - val_loss: 1.2870 - val_accuracy: 0.7297.5407 - accura - ETA: 33 - ETA: 26s - loss: 0.5507 - ac - ETA: 24s - loss: 0.5534 - accu -\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 41s 817us/sample - loss: 0.5722 - accuracy: 0.9096 - val_loss: 1.1164 - val_accuracy: 0.7431: 28s - loss: 0.5169 - - ETA: 26s - loss: - ETA: 24s - loss: 0.5286 - accura - ETA: 23s - loss: 0.5309 - accuracy: - ETA: 22s - loss: 0.5316 - accuracy: 0 - ETA: 21s - loss: 0.5331 - acc - ETA: 2s - loss: 0.5694 - accuracy:  -\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 41s 817us/sample - loss: 0.5577 - accuracy: 0.9150 - val_loss: 1.1043 - val_accuracy: 0.7711ccur - ETA: 28s - loss: 0.5044 - accurac - ETA: 27s - loss - ETA:   - ETA: 12 - ETA - ETA: 3s - loss: 0.5553 - accuracy - ETA: 3s - l - ETA: 1s - loss: 0.5570 - accura - ETA: 1s - loss: 0.5570 -  - ETA: 0s - loss: 0.5576 - \n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 41s 818us/sample - loss: 0.5464 - accuracy: 0.9189 - val_loss: 1.3504 - val_accuracy: 0.7156: 0.4855 - accuracy: 0.9 - ETA: 30s - loss: 0.4870 - ac - ETA: 29s - loss: 0.4909 - ac - ETA: 28s - loss: 0.4957 - a - ETA: 26s - ETA: 23s - loss: 0.4990  - ETA: 22s - loss: 0.50 - ETA: 20s - loss: 0.5095 - accuracy: - ETA: 19s - loss: 0.5111 - a - ETA: 17s - loss: 0.513 - ETA: 16s - loss: 0.5160 - accuracy: 0.92 - ETA: 15s - loss: 0.5164  - ETA: 14s - loss: 0.5190 - accuracy: 0. - ETA: 13s - loss: 0.5198 - accu - ETA: 12s - loss: 0.5217 - accur - ETA: 4s - loss: 0.539 - ETA: 1s - loss:\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 41s 826us/sample - loss: 0.5374 - accuracy: 0.9237 - val_loss: 1.3877 - val_accuracy: 0.7270\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 41s 824us/sample - loss: 0.5297 - accuracy: 0.9248 - val_loss: 1.1605 - val_accuracy: 0.7645\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 41s 824us/sample - loss: 0.5270 - accuracy: 0.9266 - val_loss: 1.4856 - val_accuracy: 0.7213: 10s - los - ETA: 7s - loss: 0.5 - ETA: 5s - ETA: 2s - loss: 0.525 - ETA: 1s - loss: 0 - ETA: 0s - loss: 0.5268 - accuracy: 0.\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 41s 818us/sample - loss: 0.5134 - accuracy: 0.9321 - val_loss: 1.1014 - val_accuracy: 0.7707 loss: 0.4739 - a - ETA: 31s - loss: 0.4 - ETA: 6s - loss: 0.5081  - ETA: \n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 41s 821us/sample - loss: 0.5089 - accuracy: 0.9334 - val_loss: 1.4164 - val_accuracy: 0.7210TA: 29s - loss: 0.4700 - - ETA: 28s - loss: 0.4755 - a - ETA: 26s - loss: 0.4779 - ac - - ETA: 1s - loss:\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 41s 819us/sample - loss: 0.5082 - accuracy: 0.9345 - val_loss: 1.8797 - val_accuracy: 0.6791oss: 0.5291 - accuracy - ETA: 35s - loss: 0.5164 - accuracy: 0. - ETA: 34s -  - ETA: 27s - loss: 0.4 - ETA: 25s - ETA: 22s -  - ETA: 19s - loss: 0.4933 - - ETA: 17s - loss: 0.4947 -  - ETA: 16s - loss: 0.4965 - accuracy:  - ETA: 15s - loss: 0.4974 - accuracy: - ETA: 15s - loss: 0.4981 - accuracy - ETA: 14s - lo\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 41s 817us/sample - loss: 0.5005 - accuracy: 0.9377 - val_loss: 1.7270 - val_accuracy: 0.6769 loss:  - ETA: 21s - loss: 0.4687 - accura - ETA: 20s - loss: 0.4732 - accuracy: 0 - ETA: 19s - loss: 0.4742  - ETA: 17s - loss: 0.4792 - acc - ETA: 16s - loss: 0.4817 - accuracy: 0.943 - ETA: 16s - loss: 0.4817 - accu - ETA: 15s - loss: 0.4838 -  - ETA: 13s - loss: 0.4848 - a - ETA: 12s - loss: 0.4861 - accuracy: 0.  - ETA: 0s - loss: 0.4998 - accura\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 41s 818us/sample - loss: 0.4961 - accuracy: 0.9398 - val_loss: 1.2525 - val_accuracy: 0.7552 - loss: 0.4618  - ETA: 28s - loss: 0.4613 - a - ETA: 27s - loss: 0.4625 - accuracy: 0.9 - ETA: 26s - loss: 0.4635 - accu - ETA: 25s - loss: 0.4649 - ac - ETA: 24s - loss: 0.4684  - ETA: 22s - loss: 0.4729 - accuracy: 0.94 - ETA: 22s - l - ETA: 19s - loss: 0.4750 -  - ETA: 5s - loss: 0\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 41s 819us/sample - loss: 0.4936 - accuracy: 0.9408 - val_loss: 1.2485 - val_accuracy: 0.7608loss: 0.4456 - accuracy: - ETA: 31s - loss: 0.4465 - accuracy: 0. - ETA: 31s - loss: 0.4449 - accuracy - ETA: 22s - loss: 0.45\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 41s 818us/sample - loss: 0.4894 - accuracy: 0.9416 - val_loss: 1.3322 - val_accuracy: 0.7560 loss: 0.4551 - accuracy: 0. - ETA: 31s - loss - ETA: 29s - loss: 0.4550 - accuracy - ETA: 24s - loss: 0.4588 - accurac - ETA: 23s - loss: 0.4616 - a - ETA: 21s - loss: 0.4627  - ETA: 20s - loss: 0.4641 - accuracy - ETA: 19\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 41s 817us/sample - loss: 0.4810 - accuracy: 0.9450 - val_loss: 1.4875 - val_accuracy: 0.7095 31s - loss: 0.444 - ETA: 29s - loss: 0.4397 - accuracy: 0.9 - ETA: 29s - loss: 0.4384 - accuracy: 0.961 - ETA: 29s - loss: 0.4379 - accuracy: 0.9 - ETA: 29s - loss: 0.4379 - accuracy: 0.961 - ETA: 29s - loss: 0.4374 - accuracy: 0.9 - ETA: 28s - loss: 0.4371 - accuracy: 0 - ETA: 28s - loss: 0.4373 - accuracy: 0.9 - ETA: 27s - loss: 0.4370 - accura - E - ETA: 22s - loss: 0.4405 - accuracy: 0.9 - ETA: 18s - loss: 0.4509 - accuracy: 0.9 - ETA: 18s - loss: 0.4509 - accuracy: - ETA - ETA: 1s - loss: 0\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 41s 818us/sample - loss: 0.4877 - accuracy: 0.9411 - val_loss: 1.2534 - val_accuracy: 0.7604.947 - ETA: - ETA: 30s - loss: 0.4590 - accuracy: 0.95 - ETA: 29s - loss: 0.4594 - accu - ETA: 28s - l - ETA: 25s - loss: 0.4602 - a - ETA: 24s - loss: 0.4594 - accuracy: 0.9 - ETA: 24s - loss: 0.4590 - accuracy: 0. - ETA: 23s - loss: 0.4592 - accuracy: 0 - ETA: 23s - loss: 0.4588 - accuracy: 0 - ETA: 22s - loss: 0.45 - ETA: 20s  - ETA - ETA: 1s - loss: 0.4873 - accuracy:  - ETA: 0s - loss: 0.487\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 41s 818us/sample - loss: 0.4765 - accuracy: 0.9465 - val_loss: 1.3083 - val_accuracy: 0.7551loss: 0.4467 - a - ETA: 35s - loss: 0.4428 - - ETA: 29s - loss: 0.4383 - ETA: 27s - loss: 0.4373 -  - ETA: 2 - ETA: 22s - loss: 0.4413 - accuracy - ETA: 21s - loss: - ETA: 19s - loss: 0.4423 - accuracy: 0.9 - ETA: 18s - loss: 0.4441 - accuracy:  - ETA: 5s - loss: 0.4690 - accuracy - ETA: 5s - loss: - ETA: 3s - loss: 0.4711 - accuracy - ETA: 3s - loss: 0.4716 - accuracy - ETA: 3s - loss: 0.4722 - accuracy: 0.94 - ETA: 3s - loss: 0.472\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 41s 817us/sample - loss: 0.4797 - accuracy: 0.9452 - val_loss: 1.2641 - val_accuracy: 0.76256 - ETA:  - E - ETA: 1s - loss: 0.4795 - accu - ETA: 0s - loss: 0.4794 - accu\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 41s 817us/sample - loss: 0.4745 - accuracy: 0.9466 - val_loss: 1.3613 - val_accuracy: 0.73740.4266 - a - ETA: 30s - loss: 0.4278 - accur - ETA: 29s - loss: 0.4277 - accur - ETA: 27s - loss: 0.4295 - accuracy:  - ETA: 27s - l - ETA: 24s - loss: 0.4371 - accuracy: 0 - ETA: 24s - loss: 0.4383 - accura - ETA: 23s - loss: 0.4409 - accuracy: 0 - ETA: 22s - loss: 0.4416 - accura - ETA - ETA: 17s - loss: 0.4502 - accu - ETA: 16s - loss: 0.4517 - accu - ETA: 15s - loss: 0 - ETA: 13s - loss:  - ET\n",
      "Test loss: 1.361271410369873\n",
      "Test accuracy: 0.7374\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # batch 的大小，如果出現 OOM error，請降低這個值\n",
    "num_classes = 10 # 類別的數量，Cifar 10 共有 10 個類別\n",
    "epochs = 30 # 訓練整個資料集共 30個循環\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料增強\n",
    "\n",
    "使用 ResNet 網路架構 (此處沒有使用 transfer learning，因為沒有使用 pre-trained 的 weights)\n",
    "\n",
    "學習率動態調整\n",
    "\n",
    "使用 generator 訓練\n",
    "\n",
    "本日作業的解答是參考 Keras 官方的範例 - 使用 ResNet 訓練 Cifar-10 資料集，共使用了以下技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n",
      "Learning rate:  0.001\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 16)   448         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 16)   64          conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 16)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 16)   2320        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 32, 32, 16)   64          conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 32, 32, 16)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 16)   2320        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 32, 16)   64          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 32, 16)   0           activation_28[0][0]              \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 32, 16)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 32, 16)   2320        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 32, 16)   64          conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 32, 16)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 32, 16)   2320        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 32, 16)   64          conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 32, 16)   0           activation_30[0][0]              \n",
      "                                                                 batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 32, 16)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 16)   2320        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 32, 16)   64          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 32, 16)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 16)   2320        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 32, 16)   64          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 32, 16)   0           activation_32[0][0]              \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 32, 16)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 16)   2320        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 32, 16)   64          conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 32, 16)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 16)   2320        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 32, 16)   64          conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 32, 16)   0           activation_34[0][0]              \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 16)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 32, 32, 16)   2320        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 16)   64          conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 16)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 32, 32, 16)   2320        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 16)   64          conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 32, 16)   0           activation_36[0][0]              \n",
      "                                                                 batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 16)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 32, 32, 16)   2320        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 16)   64          conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 16)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 32, 16)   2320        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 16)   64          conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 32, 32, 16)   0           activation_38[0][0]              \n",
      "                                                                 batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 32, 16)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 32, 32, 16)   2320        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 32, 16)   64          conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 32, 32, 16)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 32, 32, 16)   2320        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 32, 16)   64          conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 32, 32, 16)   0           activation_40[0][0]              \n",
      "                                                                 batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 32, 32, 16)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 32, 32, 16)   2320        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 32, 32, 16)   64          conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 32, 32, 16)   0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 32, 32, 16)   2320        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 32, 32, 16)   64          conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 32, 32, 16)   0           activation_42[0][0]              \n",
      "                                                                 batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 32, 32, 16)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 32, 32, 16)   2320        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 32, 32, 16)   64          conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 32, 32, 16)   0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 32, 32, 16)   2320        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 32, 32, 16)   64          conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 32, 32, 16)   0           activation_44[0][0]              \n",
      "                                                                 batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 32, 32, 16)   0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 16, 16, 32)   4640        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 16, 16, 32)   128         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 16, 16, 32)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 16, 16, 32)   9248        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 16, 16, 32)   544         activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 16, 16, 32)   128         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 16, 16, 32)   0           conv2d_52[0][0]                  \n",
      "                                                                 batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16, 16, 32)   0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 16, 16, 32)   9248        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 16, 16, 32)   128         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 16, 16, 32)   0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 16, 16, 32)   9248        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 16, 16, 32)   128         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 16, 16, 32)   0           activation_48[0][0]              \n",
      "                                                                 batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 16, 16, 32)   0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 32)   9248        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 16, 16, 32)   128         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16, 16, 32)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 16, 16, 32)   9248        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 16, 16, 32)   128         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 16, 16, 32)   0           activation_50[0][0]              \n",
      "                                                                 batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 16, 16, 32)   0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 32)   9248        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 16, 16, 32)   128         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 16, 16, 32)   0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 32)   9248        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16, 16, 32)   128         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 16, 16, 32)   0           activation_52[0][0]              \n",
      "                                                                 batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 16, 32)   0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 32)   9248        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 32)   128         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 32)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 16, 32)   9248        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 32)   128         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 16, 16, 32)   0           activation_54[0][0]              \n",
      "                                                                 batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 16, 32)   0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 16, 16, 32)   9248        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 16, 32)   128         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 16, 16, 32)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 16, 16, 32)   9248        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 16, 16, 32)   128         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 16, 16, 32)   0           activation_56[0][0]              \n",
      "                                                                 batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16, 16, 32)   0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 16, 32)   9248        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 16, 16, 32)   128         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 16, 16, 32)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 16, 32)   9248        activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 32)   128         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 16, 16, 32)   0           activation_58[0][0]              \n",
      "                                                                 batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 32)   0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 32)   9248        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 32)   128         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 32)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 32)   9248        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 32)   128         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 16, 16, 32)   0           activation_60[0][0]              \n",
      "                                                                 batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 32)   0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 32)   9248        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 32)   128         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 32)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 32)   9248        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 32)   128         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 16, 16, 32)   0           activation_62[0][0]              \n",
      "                                                                 batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 32)   0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 8, 64)     18496       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 8, 64)     256         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 8, 8, 64)     0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 8, 64)     36928       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 8, 64)     2112        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 8, 64)     256         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 8, 8, 64)     0           conv2d_71[0][0]                  \n",
      "                                                                 batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 8, 8, 64)     0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 64)     36928       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 8, 64)     256         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 8, 8, 64)     0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 8, 64)     36928       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 8, 64)     256         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 8, 8, 64)     0           activation_66[0][0]              \n",
      "                                                                 batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 8, 64)     0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 8, 64)     36928       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 8, 64)     256         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 8, 64)     0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 8, 64)     36928       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 8, 64)     256         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 8, 8, 64)     0           activation_68[0][0]              \n",
      "                                                                 batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 8, 64)     0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 64)     36928       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 64)     256         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 8, 64)     0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 64)     36928       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 64)     256         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 8, 8, 64)     0           activation_70[0][0]              \n",
      "                                                                 batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 64)     0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 64)     36928       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 8, 64)     256         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 8, 8, 64)     0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 64)     36928       activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 8, 64)     256         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 8, 8, 64)     0           activation_72[0][0]              \n",
      "                                                                 batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 8, 8, 64)     0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 64)     36928       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 64)     256         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 8, 8, 64)     0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 64)     36928       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 64)     256         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 8, 8, 64)     0           activation_74[0][0]              \n",
      "                                                                 batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 64)     0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 64)     36928       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 64)     256         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 64)     0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 64)     36928       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 64)     256         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 8, 8, 64)     0           activation_76[0][0]              \n",
      "                                                                 batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 64)     0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 64)     36928       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 64)     256         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 64)     0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 64)     36928       activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 64)     256         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 8, 8, 64)     0           activation_78[0][0]              \n",
      "                                                                 batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 64)     0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 64)     36928       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 64)     256         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 64)     0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 64)     36928       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 64)     256         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 8, 8, 64)     0           activation_80[0][0]              \n",
      "                                                                 batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 64)     0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 861,770\n",
      "Trainable params: 857,706\n",
      "Non-trainable params: 4,064\n",
      "__________________________________________________________________________________________________\n",
      "ResNet56v1\n",
      "Using real-time data augmentation.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Trains a ResNet on the CIFAR10 dataset.\n",
    "\n",
    "ResNet 共有兩個版本，此處解答我們使用 v1 來做訓練。\n",
    "ResNet v1:\n",
    "[Deep Residual Learning for Image Recognition\n",
    "](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "ResNet v2:\n",
    "[Identity Mappings in Deep Residual Networks\n",
    "](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 訓練用的超參數\n",
    "batch_size = 128  \n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "\n",
    "# 資料標準化的方式，此處使用減去所有影像的平均值\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "# Model parameter\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "n = 9 # 使用 ResNet-56 的網路架構\n",
    "\n",
    "# 使用的 ResNet 模型版本\n",
    "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "version = 1\n",
    "\n",
    "# 計算不同 ResNet 版本對應的網路深度，此處都是根據 paper 的定義來計算\n",
    "depth = n * 6 + 2\n",
    "\n",
    "# 模型的名稱\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "# 讀取 Cifar-10 資料集\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 影像輸入的維度\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# 先把影像縮放到 0-1 之間\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# 再減去所有影像的平均值\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean \n",
    "    x_test -= x_train_mean # 此處要注意！測試資料也是減去訓練資料的平均值來做標準化，不可以減測試資料的平均值 (因為理論上你是不能知道測試資料的平均值的！)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# 對 label 做 one-hot encoding\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# 學習率動態調整。當跑到第幾個 epcoh 時，根據設定修改學習率。這邊的數值都是參考原 paper\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "# 使用 resnet_layer 來建立我們的 ResNet 模型\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    # 建立卷積層\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    # 對輸入進行卷機，根據 conv_first 來決定 conv. bn, activation 的順序\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "# Resnet v1 共有三個 stage，每經過一次 stage，影像就會變小一半，但 channels 數量增加一倍。ResNet-20 代表共有 20 層 layers，疊越深參數越多\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # 模型的初始設置，要用多少 filters，共有幾個 residual block （組成 ResNet 的單元）\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "    \n",
    "    # 建立 Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # 先對影像做第一次卷機\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    \n",
    "    # 總共建立 3 個 stage\n",
    "    for stack in range(3):\n",
    "        # 每個 stage 建立數個 residual blocks (數量視你的層數而訂，越多層越多 block)\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tensorflow.keras.layers.add([x, y]) # 此處把 featuremaps 與 上一層的輸入加起來 (欲更了解結構需閱讀原論文)\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # 建立分類\n",
    "    # 使用 average pooling，且 size 跟 featuremaps 的 size 一樣 （相等於做 GlobalAveragePooling）\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    \n",
    "    # 接上 Dense layer 來做分類\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # 建立模型\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 建立 ResNet v1 模型\n",
    "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "# 編譯模型，使用 Adam 優化器並使用學習率動態調整的函數，０代表在第一個 epochs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)\n",
    "\n",
    "# 使用動態調整學習率\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# 使用自動降低學習率 (當 validation loss 連續 5 次沒有下降時，自動降低學習率)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "# 設定 callbacks\n",
    "callbacks = [lr_reducer, lr_scheduler]\n",
    "\n",
    "\n",
    "print('Using real-time data augmentation.')\n",
    "datagen = ImageDataGenerator(\n",
    "    # set input mean to 0 over the dataset\n",
    "    featurewise_center=False,\n",
    "    # set each sample mean to 0\n",
    "    samplewise_center=False,\n",
    "    # divide inputs by std of dataset\n",
    "    featurewise_std_normalization=False,\n",
    "    # divide each input by its std\n",
    "    samplewise_std_normalization=False,\n",
    "    # apply ZCA whitening\n",
    "    zca_whitening=False,\n",
    "    # epsilon for ZCA whitening\n",
    "    zca_epsilon=1e-06,\n",
    "    # randomly rotate images in the range (deg 0 to 180)\n",
    "    rotation_range=0,\n",
    "    # randomly shift images horizontally\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically\n",
    "    height_shift_range=0.1,\n",
    "    # set range for random shear\n",
    "    shear_range=0.,\n",
    "    # set range for random zoom\n",
    "    zoom_range=0.,\n",
    "    # set range for random channel shifts\n",
    "    channel_shift_range=0.,\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    # value used for fill_mode = \"constant\"\n",
    "    cval=0.,\n",
    "    # randomly flip images\n",
    "    horizontal_flip=True,\n",
    "    # randomly flip images\n",
    "    vertical_flip=False,\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "# 將資料送進 ImageDataGenrator 中做增強\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-eadf85d4d3ad>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 390 steps, validate on 10000 samples\n",
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 51s 131ms/step - loss: 2.1430 - accuracy: 0.4085 - val_loss: 1.8835 - val_accuracy: 0.4682\n",
      "Learning rate:  0.001\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 1.5823 - accuracy: 0.5684 - val_loss: 1.7371 - val_accuracy: 0.5209\n",
      "Learning rate:  0.001\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 1.3419 - accuracy: 0.6483 - val_loss: 1.6532 - val_accuracy: 0.5607\n",
      "Learning rate:  0.001\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 1.1746 - accuracy: 0.7033 - val_loss: 1.3416 - val_accuracy: 0.6504\n",
      "Learning rate:  0.001\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 1.0645 - accuracy: 0.7353 - val_loss: 1.3915 - val_accuracy: 0.6334\n",
      "Learning rate:  0.001\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.9805 - accuracy: 0.7601 - val_loss: 1.0984 - val_accuracy: 0.7283\n",
      "Learning rate:  0.001\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.9209 - accuracy: 0.7773 - val_loss: 1.3058 - val_accuracy: 0.6839\n",
      "Learning rate:  0.001\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.8647 - accuracy: 0.7938 - val_loss: 1.0246 - val_accuracy: 0.7539\n",
      "Learning rate:  0.001\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.8273 - accuracy: 0.8033 - val_loss: 1.1552 - val_accuracy: 0.7142\n",
      "Learning rate:  0.001\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.7919 - accuracy: 0.8123 - val_loss: 1.2874 - val_accuracy: 0.6775\n",
      "Learning rate:  0.001\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.7654 - accuracy: 0.8196 - val_loss: 0.9048 - val_accuracy: 0.7813\n",
      "Learning rate:  0.001\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.7331 - accuracy: 0.8285 - val_loss: 1.0342 - val_accuracy: 0.7442\n",
      "Learning rate:  0.001\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.7138 - accuracy: 0.8343 - val_loss: 1.0082 - val_accuracy: 0.7595\n",
      "Learning rate:  0.001\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.6903 - accuracy: 0.8410 - val_loss: 0.8084 - val_accuracy: 0.8084\n",
      "Learning rate:  0.001\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.6761 - accuracy: 0.8453 - val_loss: 0.8678 - val_accuracy: 0.7860\n",
      "Learning rate:  0.001\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.6558 - accuracy: 0.8514 - val_loss: 0.7653 - val_accuracy: 0.8223\n",
      "Learning rate:  0.001\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.6387 - accuracy: 0.8567 - val_loss: 0.9938 - val_accuracy: 0.7559\n",
      "Learning rate:  0.001\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.6233 - accuracy: 0.8607 - val_loss: 0.7388 - val_accuracy: 0.8218\n",
      "Learning rate:  0.001\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.6067 - accuracy: 0.8665 - val_loss: 1.0281 - val_accuracy: 0.7298\n",
      "Learning rate:  0.001\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.5981 - accuracy: 0.8671 - val_loss: 1.0178 - val_accuracy: 0.7507\n",
      "Learning rate:  0.001\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.5843 - accuracy: 0.8727 - val_loss: 0.9927 - val_accuracy: 0.7405\n",
      "Learning rate:  0.001\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.5774 - accuracy: 0.8745 - val_loss: 0.9168 - val_accuracy: 0.7773\n",
      "Learning rate:  0.001\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.5670 - accuracy: 0.8761 - val_loss: 0.7259 - val_accuracy: 0.8322\n",
      "Learning rate:  0.001\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.5588 - accuracy: 0.8795 - val_loss: 0.7052 - val_accuracy: 0.8355\n",
      "Learning rate:  0.001\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.5467 - accuracy: 0.8820 - val_loss: 1.0213 - val_accuracy: 0.7555\n",
      "Learning rate:  0.001\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.5318 - accuracy: 0.8886 - val_loss: 0.8408 - val_accuracy: 0.8022\n",
      "Learning rate:  0.001\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.5319 - accuracy: 0.8885 - val_loss: 0.9626 - val_accuracy: 0.7806\n",
      "Learning rate:  0.001\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 44s 112ms/step - loss: 0.5249 - accuracy: 0.8889 - val_loss: 0.7130 - val_accuracy: 0.8307\n",
      "Learning rate:  0.001\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 44s 112ms/step - loss: 0.5190 - accuracy: 0.8914 - val_loss: 0.7426 - val_accuracy: 0.8227\n",
      "Learning rate:  0.001\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 44s 112ms/step - loss: 0.5164 - accuracy: 0.8943 - val_loss: 0.8290 - val_accuracy: 0.8066\n",
      "Learning rate:  0.001\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 44s 112ms/step - loss: 0.5031 - accuracy: 0.8985 - val_loss: 0.6808 - val_accuracy: 0.8434\n",
      "Learning rate:  0.001\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 44s 112ms/step - loss: 0.4988 - accuracy: 0.8985 - val_loss: 0.7191 - val_accuracy: 0.8324\n",
      "Learning rate:  0.001\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4894 - accuracy: 0.9017 - val_loss: 0.7143 - val_accuracy: 0.8374\n",
      "Learning rate:  0.001\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4897 - accuracy: 0.9000 - val_loss: 0.7459 - val_accuracy: 0.8344\n",
      "Learning rate:  0.001\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4848 - accuracy: 0.9034 - val_loss: 0.9146 - val_accuracy: 0.7868\n",
      "Learning rate:  0.001\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4778 - accuracy: 0.9036 - val_loss: 0.6895 - val_accuracy: 0.8461\n",
      "Learning rate:  0.001\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4771 - accuracy: 0.9051 - val_loss: 0.7923 - val_accuracy: 0.8253\n",
      "Learning rate:  0.001\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4712 - accuracy: 0.9075 - val_loss: 0.6639 - val_accuracy: 0.8529\n",
      "Learning rate:  0.001\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4678 - accuracy: 0.9083 - val_loss: 0.7243 - val_accuracy: 0.8359\n",
      "Learning rate:  0.001\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4626 - accuracy: 0.9093 - val_loss: 0.8396 - val_accuracy: 0.7939\n",
      "Learning rate:  0.001\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4547 - accuracy: 0.9133 - val_loss: 0.8585 - val_accuracy: 0.8002\n",
      "Learning rate:  0.001\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4571 - accuracy: 0.9099 - val_loss: 0.6741 - val_accuracy: 0.8500\n",
      "Learning rate:  0.001\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4526 - accuracy: 0.9118 - val_loss: 0.7619 - val_accuracy: 0.8314\n",
      "Learning rate:  0.001\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4551 - accuracy: 0.9125 - val_loss: 1.1070 - val_accuracy: 0.7629\n",
      "Learning rate:  0.001\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4400 - accuracy: 0.9179 - val_loss: 1.0991 - val_accuracy: 0.7649\n",
      "Learning rate:  0.001\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4479 - accuracy: 0.9151 - val_loss: 0.8337 - val_accuracy: 0.8062\n",
      "Learning rate:  0.001\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4351 - accuracy: 0.9182 - val_loss: 0.7753 - val_accuracy: 0.8327\n",
      "Learning rate:  0.001\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4404 - accuracy: 0.9173 - val_loss: 0.9430 - val_accuracy: 0.7962\n",
      "Learning rate:  0.001\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4329 - accuracy: 0.9193 - val_loss: 0.8261 - val_accuracy: 0.8131\n",
      "Learning rate:  0.001\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4329 - accuracy: 0.9199 - val_loss: 1.0017 - val_accuracy: 0.7770\n",
      "Learning rate:  0.001\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4294 - accuracy: 0.9203 - val_loss: 0.7129 - val_accuracy: 0.8408\n",
      "Learning rate:  0.001\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4274 - accuracy: 0.9216 - val_loss: 0.6911 - val_accuracy: 0.8495\n",
      "Learning rate:  0.001\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4233 - accuracy: 0.9232 - val_loss: 0.7793 - val_accuracy: 0.8305\n",
      "Learning rate:  0.001\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4229 - accuracy: 0.9223 - val_loss: 0.7232 - val_accuracy: 0.8373\n",
      "Learning rate:  0.001\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4201 - accuracy: 0.9244 - val_loss: 0.6898 - val_accuracy: 0.8439\n",
      "Learning rate:  0.001\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4151 - accuracy: 0.9252 - val_loss: 0.6951 - val_accuracy: 0.8537\n",
      "Learning rate:  0.001\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4226 - accuracy: 0.9219 - val_loss: 0.7869 - val_accuracy: 0.8252\n",
      "Learning rate:  0.001\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4201 - accuracy: 0.9237 - val_loss: 0.7806 - val_accuracy: 0.8261\n",
      "Learning rate:  0.001\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4135 - accuracy: 0.9257 - val_loss: 0.7345 - val_accuracy: 0.8338\n",
      "Learning rate:  0.001\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4167 - accuracy: 0.9247 - val_loss: 0.7007 - val_accuracy: 0.8564\n",
      "Learning rate:  0.001\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4053 - accuracy: 0.9288 - val_loss: 0.6804 - val_accuracy: 0.8475\n",
      "Learning rate:  0.001\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4101 - accuracy: 0.9274 - val_loss: 0.8145 - val_accuracy: 0.8217\n",
      "Learning rate:  0.001\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4065 - accuracy: 0.9288 - val_loss: 0.8605 - val_accuracy: 0.8251\n",
      "Learning rate:  0.001\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4047 - accuracy: 0.9287 - val_loss: 0.7536 - val_accuracy: 0.8375\n",
      "Learning rate:  0.001\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4013 - accuracy: 0.9303 - val_loss: 0.6533 - val_accuracy: 0.8587\n",
      "Learning rate:  0.001\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3969 - accuracy: 0.9310 - val_loss: 0.6790 - val_accuracy: 0.8571\n",
      "Learning rate:  0.001\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3945 - accuracy: 0.9322 - val_loss: 0.9373 - val_accuracy: 0.7934\n",
      "Learning rate:  0.001\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.4004 - accuracy: 0.9298 - val_loss: 0.7441 - val_accuracy: 0.8368\n",
      "Learning rate:  0.001\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3934 - accuracy: 0.9338 - val_loss: 0.8845 - val_accuracy: 0.8165\n",
      "Learning rate:  0.001\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3964 - accuracy: 0.9323 - val_loss: 0.9990 - val_accuracy: 0.7954\n",
      "Learning rate:  0.001\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3950 - accuracy: 0.9314 - val_loss: 0.9440 - val_accuracy: 0.7984\n",
      "Learning rate:  0.001\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3952 - accuracy: 0.9313 - val_loss: 0.7735 - val_accuracy: 0.8340\n",
      "Learning rate:  0.001\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3964 - accuracy: 0.9311 - val_loss: 0.7286 - val_accuracy: 0.8494\n",
      "Learning rate:  0.001\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3860 - accuracy: 0.9354 - val_loss: 0.6322 - val_accuracy: 0.8684\n",
      "Learning rate:  0.001\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3869 - accuracy: 0.9360 - val_loss: 0.8816 - val_accuracy: 0.8253\n",
      "Learning rate:  0.001\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3890 - accuracy: 0.9344 - val_loss: 0.6960 - val_accuracy: 0.8577\n",
      "Learning rate:  0.001\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3862 - accuracy: 0.9344 - val_loss: 0.7559 - val_accuracy: 0.8367\n",
      "Learning rate:  0.001\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3895 - accuracy: 0.9340 - val_loss: 0.6745 - val_accuracy: 0.8569\n",
      "Learning rate:  0.001\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3782 - accuracy: 0.9381 - val_loss: 0.6561 - val_accuracy: 0.8633\n",
      "Learning rate:  0.001\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3859 - accuracy: 0.9355 - val_loss: 0.6282 - val_accuracy: 0.8704\n",
      "Learning rate:  0.001\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3857 - accuracy: 0.9346 - val_loss: 0.7242 - val_accuracy: 0.8517\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.3132 - accuracy: 0.9623 - val_loss: 0.4835 - val_accuracy: 0.9118\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2888 - accuracy: 0.9693 - val_loss: 0.4864 - val_accuracy: 0.9121\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2737 - accuracy: 0.9741 - val_loss: 0.4769 - val_accuracy: 0.9163\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2654 - accuracy: 0.9760 - val_loss: 0.4906 - val_accuracy: 0.9117\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2585 - accuracy: 0.9775 - val_loss: 0.4909 - val_accuracy: 0.9123\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2538 - accuracy: 0.9784 - val_loss: 0.4956 - val_accuracy: 0.9118\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2462 - accuracy: 0.9801 - val_loss: 0.4948 - val_accuracy: 0.9134\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2397 - accuracy: 0.9818 - val_loss: 0.4983 - val_accuracy: 0.9154\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2360 - accuracy: 0.9825 - val_loss: 0.4976 - val_accuracy: 0.9132\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2307 - accuracy: 0.9836 - val_loss: 0.5168 - val_accuracy: 0.9111\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2277 - accuracy: 0.9839 - val_loss: 0.5053 - val_accuracy: 0.9144\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2221 - accuracy: 0.9856 - val_loss: 0.5111 - val_accuracy: 0.9123\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2198 - accuracy: 0.9857 - val_loss: 0.5005 - val_accuracy: 0.9151\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2161 - accuracy: 0.9862 - val_loss: 0.5116 - val_accuracy: 0.9161\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2112 - accuracy: 0.9874 - val_loss: 0.5135 - val_accuracy: 0.9130\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2091 - accuracy: 0.9874 - val_loss: 0.5310 - val_accuracy: 0.9096\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2077 - accuracy: 0.9871 - val_loss: 0.5077 - val_accuracy: 0.9138\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2054 - accuracy: 0.9878 - val_loss: 0.5092 - val_accuracy: 0.9171\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.2027 - accuracy: 0.9879 - val_loss: 0.5157 - val_accuracy: 0.9128\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1988 - accuracy: 0.9892 - val_loss: 0.5254 - val_accuracy: 0.9137\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1974 - accuracy: 0.9888 - val_loss: 0.5041 - val_accuracy: 0.9189\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1932 - accuracy: 0.9896 - val_loss: 0.5175 - val_accuracy: 0.9154\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1926 - accuracy: 0.9894 - val_loss: 0.5221 - val_accuracy: 0.9163\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/200\n",
      "390/390 [==============================] - 44s 114ms/step - loss: 0.1907 - accuracy: 0.9899 - val_loss: 0.5344 - val_accuracy: 0.9125\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1873 - accuracy: 0.9901 - val_loss: 0.5280 - val_accuracy: 0.9148\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1863 - accuracy: 0.9896 - val_loss: 0.5282 - val_accuracy: 0.9169\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1843 - accuracy: 0.9901 - val_loss: 0.5393 - val_accuracy: 0.9106\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1807 - accuracy: 0.9909 - val_loss: 0.5402 - val_accuracy: 0.9101\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1799 - accuracy: 0.9910 - val_loss: 0.5446 - val_accuracy: 0.9123\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/200\n",
      "390/390 [==============================] - 44s 114ms/step - loss: 0.1787 - accuracy: 0.9905 - val_loss: 0.5522 - val_accuracy: 0.9117\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1782 - accuracy: 0.9910 - val_loss: 0.5323 - val_accuracy: 0.9116\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1758 - accuracy: 0.9915 - val_loss: 0.5603 - val_accuracy: 0.9105\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1733 - accuracy: 0.9915 - val_loss: 0.5294 - val_accuracy: 0.9134\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1712 - accuracy: 0.9924 - val_loss: 0.5344 - val_accuracy: 0.9134\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1692 - accuracy: 0.9926 - val_loss: 0.5495 - val_accuracy: 0.9123\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1689 - accuracy: 0.9916 - val_loss: 0.5358 - val_accuracy: 0.9125\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1662 - accuracy: 0.9928 - val_loss: 0.5250 - val_accuracy: 0.9157\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1670 - accuracy: 0.9919 - val_loss: 0.5509 - val_accuracy: 0.9136\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1655 - accuracy: 0.9925 - val_loss: 0.5470 - val_accuracy: 0.9148\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1638 - accuracy: 0.9924 - val_loss: 0.5469 - val_accuracy: 0.9125\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1589 - accuracy: 0.9942 - val_loss: 0.5252 - val_accuracy: 0.9174\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1572 - accuracy: 0.9944 - val_loss: 0.5255 - val_accuracy: 0.9185\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1561 - accuracy: 0.9951 - val_loss: 0.5204 - val_accuracy: 0.9182\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1552 - accuracy: 0.9953 - val_loss: 0.5219 - val_accuracy: 0.9190\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1542 - accuracy: 0.9958 - val_loss: 0.5210 - val_accuracy: 0.9177\n",
      "Learning rate:  1e-05\n",
      "Epoch 127/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1538 - accuracy: 0.9954 - val_loss: 0.5230 - val_accuracy: 0.9180\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1535 - accuracy: 0.9956 - val_loss: 0.5243 - val_accuracy: 0.9176\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1523 - accuracy: 0.9964 - val_loss: 0.5249 - val_accuracy: 0.9180\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1528 - accuracy: 0.9959 - val_loss: 0.5240 - val_accuracy: 0.9179\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1528 - accuracy: 0.9961 - val_loss: 0.5239 - val_accuracy: 0.9185\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1522 - accuracy: 0.9958 - val_loss: 0.5274 - val_accuracy: 0.9178\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1507 - accuracy: 0.9964 - val_loss: 0.5293 - val_accuracy: 0.9181\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1511 - accuracy: 0.9963 - val_loss: 0.5276 - val_accuracy: 0.9188\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1509 - accuracy: 0.9960 - val_loss: 0.5293 - val_accuracy: 0.9187\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1495 - accuracy: 0.9971 - val_loss: 0.5278 - val_accuracy: 0.9180\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1508 - accuracy: 0.9958 - val_loss: 0.5296 - val_accuracy: 0.9175\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1499 - accuracy: 0.9963 - val_loss: 0.5327 - val_accuracy: 0.9173\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1492 - accuracy: 0.9965 - val_loss: 0.5313 - val_accuracy: 0.9178\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1494 - accuracy: 0.9968 - val_loss: 0.5332 - val_accuracy: 0.9185\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1494 - accuracy: 0.9965 - val_loss: 0.5349 - val_accuracy: 0.9186\n",
      "Learning rate:  1e-05\n",
      "Epoch 142/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1484 - accuracy: 0.9970 - val_loss: 0.5363 - val_accuracy: 0.9172\n",
      "Learning rate:  1e-05\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1473 - accuracy: 0.9972 - val_loss: 0.5380 - val_accuracy: 0.9185\n",
      "Learning rate:  1e-05\n",
      "Epoch 144/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1479 - accuracy: 0.9966 - val_loss: 0.5371 - val_accuracy: 0.9179\n",
      "Learning rate:  1e-05\n",
      "Epoch 145/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1466 - accuracy: 0.9974 - val_loss: 0.5363 - val_accuracy: 0.9181\n",
      "Learning rate:  1e-05\n",
      "Epoch 146/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1475 - accuracy: 0.9970 - val_loss: 0.5406 - val_accuracy: 0.9184\n",
      "Learning rate:  1e-05\n",
      "Epoch 147/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1472 - accuracy: 0.9967 - val_loss: 0.5397 - val_accuracy: 0.9185\n",
      "Learning rate:  1e-05\n",
      "Epoch 148/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1470 - accuracy: 0.9969 - val_loss: 0.5401 - val_accuracy: 0.9190\n",
      "Learning rate:  1e-05\n",
      "Epoch 149/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1463 - accuracy: 0.9969 - val_loss: 0.5432 - val_accuracy: 0.9194\n",
      "Learning rate:  1e-05\n",
      "Epoch 150/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1456 - accuracy: 0.9972 - val_loss: 0.5408 - val_accuracy: 0.9190\n",
      "Learning rate:  1e-05\n",
      "Epoch 151/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1458 - accuracy: 0.9971 - val_loss: 0.5460 - val_accuracy: 0.9178\n",
      "Learning rate:  1e-05\n",
      "Epoch 152/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1459 - accuracy: 0.9969 - val_loss: 0.5464 - val_accuracy: 0.9181\n",
      "Learning rate:  1e-05\n",
      "Epoch 153/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1459 - accuracy: 0.9969 - val_loss: 0.5457 - val_accuracy: 0.9186\n",
      "Learning rate:  1e-05\n",
      "Epoch 154/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1463 - accuracy: 0.9968 - val_loss: 0.5472 - val_accuracy: 0.9182\n",
      "Learning rate:  1e-05\n",
      "Epoch 155/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1447 - accuracy: 0.9971 - val_loss: 0.5519 - val_accuracy: 0.9180\n",
      "Learning rate:  1e-05\n",
      "Epoch 156/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1444 - accuracy: 0.9976 - val_loss: 0.5547 - val_accuracy: 0.9173\n",
      "Learning rate:  1e-05\n",
      "Epoch 157/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1462 - accuracy: 0.9967 - val_loss: 0.5550 - val_accuracy: 0.9167\n",
      "Learning rate:  1e-05\n",
      "Epoch 158/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1440 - accuracy: 0.9974 - val_loss: 0.5528 - val_accuracy: 0.9183\n",
      "Learning rate:  1e-05\n",
      "Epoch 159/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1448 - accuracy: 0.9968 - val_loss: 0.5540 - val_accuracy: 0.9184\n",
      "Learning rate:  1e-05\n",
      "Epoch 160/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1448 - accuracy: 0.9971 - val_loss: 0.5535 - val_accuracy: 0.9184\n",
      "Learning rate:  1e-05\n",
      "Epoch 161/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1448 - accuracy: 0.9968 - val_loss: 0.5521 - val_accuracy: 0.9182\n",
      "Learning rate:  1e-06\n",
      "Epoch 162/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1437 - accuracy: 0.9971 - val_loss: 0.5515 - val_accuracy: 0.9189\n",
      "Learning rate:  1e-06\n",
      "Epoch 163/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1437 - accuracy: 0.9972 - val_loss: 0.5519 - val_accuracy: 0.9183\n",
      "Learning rate:  1e-06\n",
      "Epoch 164/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1441 - accuracy: 0.9971 - val_loss: 0.5531 - val_accuracy: 0.9182\n",
      "Learning rate:  1e-06\n",
      "Epoch 165/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1434 - accuracy: 0.9973 - val_loss: 0.5513 - val_accuracy: 0.9184\n",
      "Learning rate:  1e-06\n",
      "Epoch 166/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1447 - accuracy: 0.9970 - val_loss: 0.5520 - val_accuracy: 0.9184\n",
      "Learning rate:  1e-06\n",
      "Epoch 167/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1436 - accuracy: 0.9973 - val_loss: 0.5524 - val_accuracy: 0.9179\n",
      "Learning rate:  1e-06\n",
      "Epoch 168/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1435 - accuracy: 0.9977 - val_loss: 0.5515 - val_accuracy: 0.9179\n",
      "Learning rate:  1e-06\n",
      "Epoch 169/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1436 - accuracy: 0.9971 - val_loss: 0.5521 - val_accuracy: 0.9180\n",
      "Learning rate:  1e-06\n",
      "Epoch 170/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1435 - accuracy: 0.9973 - val_loss: 0.5520 - val_accuracy: 0.9184\n",
      "Learning rate:  1e-06\n",
      "Epoch 171/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1436 - accuracy: 0.9973 - val_loss: 0.5519 - val_accuracy: 0.9178\n",
      "Learning rate:  1e-06\n",
      "Epoch 172/200\n",
      "390/390 [==============================] - 44s 114ms/step - loss: 0.1432 - accuracy: 0.9976 - val_loss: 0.5526 - val_accuracy: 0.9184\n",
      "Learning rate:  1e-06\n",
      "Epoch 173/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1430 - accuracy: 0.9974 - val_loss: 0.5514 - val_accuracy: 0.9188\n",
      "Learning rate:  1e-06\n",
      "Epoch 174/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1433 - accuracy: 0.9974 - val_loss: 0.5517 - val_accuracy: 0.9187\n",
      "Learning rate:  1e-06\n",
      "Epoch 175/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1437 - accuracy: 0.9972 - val_loss: 0.5512 - val_accuracy: 0.9185\n",
      "Learning rate:  1e-06\n",
      "Epoch 176/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1432 - accuracy: 0.9974 - val_loss: 0.5513 - val_accuracy: 0.9184\n",
      "Learning rate:  1e-06\n",
      "Epoch 177/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1428 - accuracy: 0.9974 - val_loss: 0.5528 - val_accuracy: 0.9183\n",
      "Learning rate:  1e-06\n",
      "Epoch 178/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1431 - accuracy: 0.9975 - val_loss: 0.5520 - val_accuracy: 0.9183\n",
      "Learning rate:  1e-06\n",
      "Epoch 179/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1428 - accuracy: 0.9976 - val_loss: 0.5519 - val_accuracy: 0.9181\n",
      "Learning rate:  1e-06\n",
      "Epoch 180/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1426 - accuracy: 0.9978 - val_loss: 0.5532 - val_accuracy: 0.9183\n",
      "Learning rate:  1e-06\n",
      "Epoch 181/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1428 - accuracy: 0.9975 - val_loss: 0.5522 - val_accuracy: 0.9183\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1422 - accuracy: 0.9976 - val_loss: 0.5518 - val_accuracy: 0.9184\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1428 - accuracy: 0.9974 - val_loss: 0.5526 - val_accuracy: 0.9180\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1435 - accuracy: 0.9973 - val_loss: 0.5530 - val_accuracy: 0.9185\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1428 - accuracy: 0.9975 - val_loss: 0.5520 - val_accuracy: 0.9182\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1427 - accuracy: 0.9974 - val_loss: 0.5528 - val_accuracy: 0.9179\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1437 - accuracy: 0.9970 - val_loss: 0.5519 - val_accuracy: 0.9184\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1429 - accuracy: 0.9976 - val_loss: 0.5527 - val_accuracy: 0.9180\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1431 - accuracy: 0.9972 - val_loss: 0.5530 - val_accuracy: 0.9180\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/200\n",
      "390/390 [==============================] - 44s 114ms/step - loss: 0.1431 - accuracy: 0.9973 - val_loss: 0.5517 - val_accuracy: 0.9182\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1429 - accuracy: 0.9973 - val_loss: 0.5527 - val_accuracy: 0.9186\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/200\n",
      "390/390 [==============================] - 44s 114ms/step - loss: 0.1431 - accuracy: 0.9974 - val_loss: 0.5526 - val_accuracy: 0.9186\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/200\n",
      "390/390 [==============================] - 44s 114ms/step - loss: 0.1426 - accuracy: 0.9976 - val_loss: 0.5529 - val_accuracy: 0.9180\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1418 - accuracy: 0.9979 - val_loss: 0.5529 - val_accuracy: 0.9184\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1426 - accuracy: 0.9975 - val_loss: 0.5531 - val_accuracy: 0.9187\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1426 - accuracy: 0.9975 - val_loss: 0.5528 - val_accuracy: 0.9177\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1431 - accuracy: 0.9975 - val_loss: 0.5530 - val_accuracy: 0.9185\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1419 - accuracy: 0.9978 - val_loss: 0.5536 - val_accuracy: 0.9181\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/200\n",
      "390/390 [==============================] - 44s 114ms/step - loss: 0.1430 - accuracy: 0.9971 - val_loss: 0.5532 - val_accuracy: 0.9185\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/200\n",
      "390/390 [==============================] - 44s 113ms/step - loss: 0.1425 - accuracy: 0.9973 - val_loss: 0.5534 - val_accuracy: 0.9181\n",
      "10000/10000 [==============================] - 4s 408us/sample - loss: 0.5534 - accuracy: 0.9181\n",
      "Test loss: 0.5533585595607757\n",
      "Test accuracy: 0.9181\n"
     ]
    }
   ],
   "source": [
    "# 訓練模型囉！\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=int(len(x_train)//batch_size),\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=epochs, verbose=1, workers=4,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# 評估我們的模型\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
